---
output:
  html_document:
    highlight: null
    css: style.css
    self_contained: no
    keep_md: true
---





<br>

<h3 class=pfblock-header> The Computational Social Science Workshop Presents </h3>

<h1 class=pfblock-header3> Aylin Caliskan</h1>
<h3 class=pfblock-header3> School of Engineering & Applied Science </h3>
<h3 class=pfblock-header3> George Washington University </h3>

<br><br>



<p class=pfblock-header3>The <a href="https://macss.uchicago.edu/content/computation-workshop"> Computational Social Science Workshop </a> at the University of Chicago cordially invites you to attend this week's talk:</p>



<br>

<div class=pfblock-header3>
<h2 class=pfblock-header>
  <a href=https://github.com/uchicago-computation-workshop/Winter2021/tree/master/01-14_Caliskan> Artificial Intelligence for Social Good: When Machines Learn Human-like Biases from Data </a>
</h2>

<br>
</div>



<p class=footertext2>

**Summary:** Developing machine learning methods theoretically grounded in implicit social cognition reveals that unsupervised machine learning captures associations, including human-like biases, objective facts, and historical information, from the hidden patterns in datasets. Machines that learn representations of language from corpora embed human-like biases from the statistical regularities of language. Similarly, image representations in computer vision contain human-like biases due to stereotypical portrayals in vision datasets. On the one hand, the principled methodology for measuring associations in artificial intelligence provides a systematic approach to study society, language, vision, and learning. On the other hand, downstream artificial intelligence applications built on general-purpose representations are accelerating decision-making processes ranging from employment and college admissions to law enforcement and content moderation. As these applications are shaping society while making consequential decisions about individuals, open problems remain regarding the propagation and mitigation of biases in the expanding machine learning pipeline.


</p>

<br>

<h4 class=pfblock-header3> Thursday, 01/14/2021 </h4>
<h4 class=pfblock-header3> 11:20am-12:40pm </h4>

<br><br>

<p class=footertext2>

**Aylin Caliskan** is an assistant professor of computer science at The George Washington University. Her research interests include the emerging science of bias in machine learning, fairness in artificial intelligence, data privacy, and security. Her work aims to characterize and quantify aspects of natural and artificial intelligence using a multitude of machine learning and language processing techniques. In her recent publication in Science, she demonstrated how semantics derived from language corpora contain human-like biases. Prior to that, she developed novel privacy attacks to de-anonymize programmers using code stylometry. 
</p>

<br>

<p class=footertext2>
**Registration**: The presentation will be held virtually on Zoom. For security purposes, please register through this [link](https://uchicago.zoom.us/meeting/register/tJAlc-qhqDssG9cLUWqkzvRMqp-68pSgLnfb) to request access. Only accounts affiliated with the University of Chicago will be granted access.
</p>

<br>
This week's suggested readings:

- [Caliskan, A., Bryson, J. J., & Narayanan, A. (2017). Semantics derived automatically from language corpora contain human-like biases. Science, 356(6334), 183-186.](https://github.com/uchicago-computation-workshop/Winter2021/blob/master/01-14_Caliskan/Caliskan,Bryson&Narayanan(2017).pdf)
- [Guo, W. & Caliskan, A. (2020). Detecting Emergent Intersectional Biases: Contextualized Word Embeddings Contain a Distribution of Human-like Biases.](https://github.com/uchicago-computation-workshop/Winter2021/blob/master/01-14_Caliskan/Guo&Caliskan(2020).pdf)

Additional readings:

- [Guo, W. & Caliskan, A. (2020). Detecting Emergent Intersectional Biases: Contextualized Word Embeddings Contain a Distribution of Human-like Biases.](https://github.com/uchicago-computation-workshop/Winter2021/blob/master/01-14_Caliskan/Guo&Caliskan(2020).pdf)
- [Toney, A. & Caliskan, A. (2020). ValNorm: A New Word Embedding Intrinsic Evaluation Method Reveals Valence Biases are Consistent Across Languages and Over Decades.](https://github.com/uchicago-computation-workshop/Winter2021/blob/master/01-14_Caliskan/Toney&Caliskan(2020).pdf)

<br>

<br><br>

---

<p class=footertext> The 2020-2021 <a href="https://macss.uchicago.edu/content/computation-workshop"> Computational Social Science Workshop </a> meets each Thursday from 11:20 a.m. to 12:40 p.m.. All interested faculty and graduate students are welcome.</p>



<p class=footertext>Students in the Masters of Computational Social Science program are expected to attend and join the discussion by posting a comment on the <a href=https://github.com/uchicago-computation-workshop/Winter2021/issues/1>issues page</a> of the <a href=https://github.com/uchicago-computation-workshop/Winter2021/tree/master/01-14_Caliskan>workshop's public repository on GitHub.</a></p>
